# --- START OF FILE src/realtime_detector.py ---

import cv2
import numpy as np
from tensorflow.keras.models import load_model
from ultralytics import YOLO
from collections import deque

# ================= é…ç½®åŒº =================
EMOTION_MODEL_PATH = 'models/resnet50_emotion_model.keras'
YOLO_MODEL_NAME = 'yolov8n-face.pt'
EMOTIONS = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']
IMG_SIZE = 48

# --- æ ¸å¿ƒä¼˜åŒ–é…ç½® ---
# 1. é™ä½æ£€æµ‹é—¨æ§›ï¼šè§£å†³â€œè¯†åˆ«ä¸å‡ºæ¥â€çš„é—®é¢˜
YOLO_CONF_THRESHOLD = 0.4  

# 2. è®°å¿†ç¼“å†²ï¼šè§£å†³â€œIDä¹±è·³â€çš„é—®é¢˜
# å…è®¸äººè„¸ä¸¢å¤±å¤šå°‘å¸§åæ‰æ³¨é”€ IDï¼Ÿ(è®¾ä¸º 10 å¸§ï¼Œçº¦ 0.3 ç§’)
MAX_DISAPPEARED = 10 

# 3. æƒ…ç»ªå¹³æ»‘çª—å£
SMOOTH_WINDOW = 8 

# ==========================================

def calculate_iou(boxA, boxB):
    """è®¡ç®—ä¸¤ä¸ªçŸ©å½¢æ¡†çš„é‡å ç‡ (Intersection over Union)"""
    xA = max(boxA[0], boxB[0])
    yA = max(boxA[1], boxB[1])
    xB = min(boxA[2], boxB[2])
    yB = min(boxA[3], boxB[3])

    interArea = max(0, xB - xA) * max(0, yB - yA)
    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])
    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])

    iou = interArea / float(boxAArea + boxBArea - interArea + 1e-6)
    return iou

class AdvancedFaceTracker:
    def __init__(self):
        # å­˜å‚¨: { id: {'box': [x1,y1,x2,y2], 'disappeared': 0, 'probs': deque, 'label': '..'} }
        self.tracked_objects = {} 
        self.next_object_id = 0

    def register(self, box):
        """æ³¨å†Œæ–° ID"""
        self.tracked_objects[self.next_object_id] = {
            'box': box,
            'disappeared': 0,
            'probs': deque(maxlen=SMOOTH_WINDOW), # æƒ…ç»ªå¹³æ»‘é˜Ÿåˆ—
            'current_label': 'Detecting...',
            'current_conf': 0.0
        }
        self.next_object_id += 1

    def deregister(self, object_id):
        """æ³¨é”€ ID"""
        del self.tracked_objects[object_id]

    def update(self, rects):
        """
        æ ¸å¿ƒè¿½è¸ªé€»è¾‘ï¼šåŸºäº IoU çš„åŒ¹é…
        rects: å½“å‰å¸§æ‰€æœ‰æ£€æµ‹åˆ°çš„æ¡†
        """
        # 1. å¦‚æœæ²¡æœ‰æ­£åœ¨è¿½è¸ªçš„å¯¹è±¡ï¼Œå…¨éƒ¨æ³¨å†Œä¸ºæ–°å¯¹è±¡
        if len(self.tracked_objects) == 0:
            for rect in rects:
                self.register(rect)
            return self.tracked_objects

        # 2. å‡†å¤‡æ•°æ®
        object_ids = list(self.tracked_objects.keys())
        object_values = list(self.tracked_objects.values())
        tracked_boxes = [obj['box'] for obj in object_values]

        # 3. å¦‚æœå½“å‰å¸§æ²¡æœ‰æ£€æµ‹åˆ°äººè„¸ï¼Œæ‰€æœ‰äºº disappeared + 1
        if len(rects) == 0:
            for object_id in object_ids:
                self.tracked_objects[object_id]['disappeared'] += 1
                if self.tracked_objects[object_id]['disappeared'] > MAX_DISAPPEARED:
                    self.deregister(object_id)
            return self.tracked_objects

        # 4. è®¡ç®— IoU çŸ©é˜µ (æ—§æ¡† vs æ–°æ¡†)
        # è¿™æ˜¯ä¸€ä¸ªç®€å•çš„è´ªå©ªåŒ¹é…é€»è¾‘
        used_rows = set() # å·²åŒ¹é…çš„æ—§ ID ç´¢å¼•
        used_cols = set() # å·²åŒ¹é…çš„æ–°æ¡†ç´¢å¼•

        # è®¡ç®—æ‰€æœ‰å¯èƒ½çš„ IoU
        matches = []
        for i, old_box in enumerate(tracked_boxes):
            for j, new_box in enumerate(rects):
                iou = calculate_iou(old_box, new_box)
                if iou > 0.3: # åªæœ‰é‡å ç‡ > 30% æ‰è®¤ä¸ºæ˜¯åŒä¸€ä¸ª
                    matches.append((iou, i, j))
        
        # æŒ‰ IoU ä»å¤§åˆ°å°æ’åºï¼Œä¼˜å…ˆåŒ¹é…é‡å åº¦æœ€é«˜çš„
        matches.sort(key=lambda x: x[0], reverse=True)

        for iou, row, col in matches:
            if row in used_rows or col in used_cols:
                continue

            # åŒ¹é…æˆåŠŸï¼šæ›´æ–°æ¡†ï¼Œé‡ç½®æ¶ˆå¤±è®¡æ•°
            object_id = object_ids[row]
            self.tracked_objects[object_id]['box'] = rects[col]
            self.tracked_objects[object_id]['disappeared'] = 0
            
            used_rows.add(row)
            used_cols.add(col)

        # 5. å¤„ç†æœªåŒ¹é…çš„æ—§ ID (è®¤ä¸ºæš‚æ—¶æ¶ˆå¤±)
        for i in range(len(object_ids)):
            if i not in used_rows:
                object_id = object_ids[i]
                self.tracked_objects[object_id]['disappeared'] += 1
                if self.tracked_objects[object_id]['disappeared'] > MAX_DISAPPEARED:
                    self.deregister(object_id)

        # 6. å¤„ç†æœªåŒ¹é…çš„æ–°æ¡† (æ³¨å†Œä¸ºæ–° ID)
        for i in range(len(rects)):
            if i not in used_cols:
                self.register(rects[i])

        return self.tracked_objects

# ================= åˆå§‹åŒ– =================
print("--- åˆå§‹åŒ–å¢å¼ºç‰ˆç³»ç»Ÿ ---")
try:
    # å°è¯•åŠ è½½å¢å¼ºæ¨¡å‹ï¼Œå¦‚æœæ²¡æœ‰å°±åŠ è½½æ™®é€šæ¨¡å‹
    try:
        emotion_model = load_model('models/best_emotion_model_enhanced.keras')
        print("âœ… åŠ è½½äº†å¢å¼ºç‰ˆæƒ…ç»ªæ¨¡å‹ (Enhanced)")
    except:
        emotion_model = load_model(EMOTION_MODEL_PATH)
        print("âš ï¸ åŠ è½½äº†æ™®é€šç‰ˆæƒ…ç»ªæ¨¡å‹ (Standard)")
        
    face_model = YOLO(YOLO_MODEL_NAME)
    print("âœ… YOLOv8 æ£€æµ‹ç³»ç»Ÿå°±ç»ª (ä½é˜ˆå€¼æ¨¡å¼)")
except Exception as e:
    print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
    exit()

tracker = AdvancedFaceTracker()
cap = cv2.VideoCapture(0)

print(f"\nğŸš€ ç³»ç»Ÿè¿è¡Œä¸­ | æ£€æµ‹é˜ˆå€¼: {YOLO_CONF_THRESHOLD} | IDè®°å¿†: {MAX_DISAPPEARED}å¸§")
print("æŒ‰ 'q' é€€å‡º")

while True:
    ret, frame = cap.read()
    if not ret: break
    frame = cv2.flip(frame, 1) # é•œåƒ
    
    # 1. YOLO æ£€æµ‹ (ä½¿ç”¨æ›´ä½çš„ conf é˜ˆå€¼)
    results = face_model(frame, verbose=False, conf=YOLO_CONF_THRESHOLD)
    
    rects = []
    if results[0].boxes:
        # åªå–åæ ‡ï¼Œè½¬ä¸º int åˆ—è¡¨
        boxes = results[0].boxes.data.cpu().numpy()
        for box in boxes:
            x1, y1, x2, y2 = map(int, box[:4])
            rects.append([x1, y1, x2, y2])

    # 2. è¿½è¸ªå™¨æ›´æ–° (æ ¸å¿ƒ)
    # è¿™ä¸€æ­¥è¿”å›çš„æ˜¯æ‰€æœ‰"æ´»ç€çš„" IDï¼ŒåŒ…æ‹¬çŸ­æš‚æ¶ˆå¤±ä½†è¿˜åœ¨è®°å¿†é‡Œçš„
    objects = tracker.update(rects)

    # 3. éå†å¤„ç†
    for obj_id, data in objects.items():
        # å¦‚æœè¿™ä¸ª ID å½“å‰å¤„äº"æ¶ˆå¤±ä¸­"çŠ¶æ€ (disappeared > 0)ï¼Œå°±ä¸ç”»æ¡†ï¼Œä¹Ÿä¸é¢„æµ‹
        if data['disappeared'] > 0:
            continue

        x1, y1, x2, y2 = data['box']
        
        # --- é¢„å¤„ç† (å¢åŠ  Paddingï¼Œè§£å†³å¤§å¤´ç…§é—®é¢˜) ---
        h_img, w_img, _ = frame.shape
        pad = int((y2 - y1) * 0.2) # 20% Padding
        x1_p = max(0, x1 - pad)
        y1_p = max(0, y1 - pad)
        x2_p = min(w_img, x2 + pad)
        y2_p = min(h_img, y2 + pad)
        
        face_roi = frame[y1_p:y2_p, x1_p:x2_p]
        
        if face_roi.size > 0:
            # --- æƒ…ç»ªé¢„æµ‹ ---
            try:
                gray = cv2.cvtColor(face_roi, cv2.COLOR_BGR2GRAY)
                resized = cv2.resize(gray, (IMG_SIZE, IMG_SIZE))
                normalized = resized.astype('float32') / 255.0
                input_data = np.expand_dims(np.expand_dims(normalized, -1), 0)

                # é¢„æµ‹
                preds = emotion_model.predict(input_data, verbose=0)[0]
                
                # --- å¹³æ»‘å¤„ç† ---
                data['probs'].append(preds)
                avg_preds = np.mean(data['probs'], axis=0)
                
                idx = np.argmax(avg_preds)
                data['current_label'] = EMOTIONS[idx]
                data['current_conf'] = avg_preds[idx]

            except Exception:
                pass

        # --- ç»˜åˆ¶ UI ---
        label = data['current_label']
        conf = data['current_conf']
        
        # æ ¹æ®æƒ…ç»ªå˜è‰²
        color = (0, 255, 0) # é»˜è®¤ç»¿
        if label in ['Angry', 'Disgust', 'Fear', 'Sad']: 
            color = (0, 0, 255) # çº¢è‰²
        elif label == 'Happy':
            color = (0, 255, 255) # é»„è‰²
            
        # ç»˜åˆ¶äººè„¸æ¡†
        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
        
        # ç»˜åˆ¶èƒŒæ™¯æ¡è®©æ–‡å­—æ›´æ¸…æ¥š
        info_text = f"ID:{obj_id} {label} {int(conf*100)}%"
        t_size = cv2.getTextSize(info_text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)[0]
        cv2.rectangle(frame, (x1, y1 - 25), (x1 + t_size[0], y1), color, -1)
        
        # ç»˜åˆ¶æ–‡å­—
        cv2.putText(frame, info_text, (x1, y1 - 7), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)

    cv2.imshow('Pro Emotion Detector', frame)
    if cv2.waitKey(1) == ord('q'): break

cap.release()
cv2.destroyAllWindows()