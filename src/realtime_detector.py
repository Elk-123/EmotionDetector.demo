import cv2
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import load_model
from ultralytics import YOLO
from collections import deque
import os

# ================= åŸºç¡€é…ç½®ï¼ˆæ— éœ€æ‰‹åŠ¨ä¿®æ”¹ï¼‰=================
EMOTION_MODEL_PATH = 'models/resnet50_rafdb_model.keras'
YOLO_MODEL_NAME = 'yolov8n.pt'
EMOTIONS = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']
tf.keras.config.enable_unsafe_deserialization()

# --- æ ¸å¿ƒä¼˜åŒ–é…ç½® ---
YOLO_CONF_THRESHOLD = 0.4  
MAX_DISAPPEARED = 10 
SMOOTH_WINDOW = 8 

# ==========================================

def calculate_iou(boxA, boxB):
    """è®¡ç®—ä¸¤ä¸ªçŸ©å½¢æ¡†çš„é‡å ç‡ (Intersection over Union)"""
    xA = max(boxA[0], boxB[0])
    yA = max(boxA[1], boxB[1])
    xB = min(boxA[2], boxB[2])
    yB = min(boxA[3], boxB[3])

    interArea = max(0, xB - xA) * max(0, yB - yA)
    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])
    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])

    iou = interArea / float(boxAArea + boxBArea - interArea + 1e-6)
    return iou

class AdvancedFaceTracker:
    def __init__(self):
        self.tracked_objects = {} 
        self.next_object_id = 0

    def register(self, box):
        """æ³¨å†Œæ–° ID"""
        self.tracked_objects[self.next_object_id] = {
            'box': box,
            'disappeared': 0,
            'probs': deque(maxlen=SMOOTH_WINDOW),
            'current_label': 'Detecting...',
            'current_conf': 0.0
        }
        self.next_object_id += 1

    def deregister(self, object_id):
        """æ³¨é”€ ID"""
        del self.tracked_objects[object_id]

    def update(self, rects):
        """æ ¸å¿ƒè¿½è¸ªé€»è¾‘ï¼šåŸºäº IoU çš„åŒ¹é…"""
        if len(self.tracked_objects) == 0:
            for rect in rects:
                self.register(rect)
            return self.tracked_objects

        object_ids = list(self.tracked_objects.keys())
        object_values = list(self.tracked_objects.values())
        tracked_boxes = [obj['box'] for obj in object_values]

        if len(rects) == 0:
            for object_id in object_ids:
                self.tracked_objects[object_id]['disappeared'] += 1
                if self.tracked_objects[object_id]['disappeared'] > MAX_DISAPPEARED:
                    self.deregister(object_id)
            return self.tracked_objects

        used_rows = set()
        used_cols = set()
        matches = []

        for i, old_box in enumerate(tracked_boxes):
            for j, new_box in enumerate(rects):
                iou = calculate_iou(old_box, new_box)
                if iou > 0.3:
                    matches.append((iou, i, j))
        
        matches.sort(key=lambda x: x[0], reverse=True)

        for iou, row, col in matches:
            if row in used_rows or col in used_cols:
                continue
            object_id = object_ids[row]
            self.tracked_objects[object_id]['box'] = rects[col]
            self.tracked_objects[object_id]['disappeared'] = 0
            used_rows.add(row)
            used_cols.add(col)

        for i in range(len(object_ids)):
            if i not in used_rows:
                object_id = object_ids[i]
                self.tracked_objects[object_id]['disappeared'] += 1
                if self.tracked_objects[object_id]['disappeared'] > MAX_DISAPPEARED:
                    self.deregister(object_id)

        for i in range(len(rects)):
            if i not in used_cols:
                self.register(rects[i])

        return self.tracked_objects

# ================= æ™ºèƒ½åˆå§‹åŒ–ï¼ˆè‡ªåŠ¨é€‚é…æ¨¡å‹è¾“å…¥ï¼‰=================
print("--- åˆå§‹åŒ–å¢å¼ºç‰ˆç³»ç»Ÿ ---")
tf.keras.config.enable_unsafe_deserialization()

try:
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    if not os.path.exists(EMOTION_MODEL_PATH):
        raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°: {EMOTION_MODEL_PATH}")
    
    # åŠ è½½æ¨¡å‹å¹¶è‡ªåŠ¨è¯†åˆ«è¾“å…¥é…ç½®
    emotion_model = load_model(EMOTION_MODEL_PATH, compile=False)
    model_input_shape = emotion_model.input_shape
    print(f"âœ… æˆåŠŸåŠ è½½ ResNet50 æ¨¡å‹")
    print(f"ğŸ“Œ æ¨¡å‹æœŸæœ›è¾“å…¥å½¢çŠ¶: {model_input_shape}")
    
    # è‡ªåŠ¨æå–è¾“å…¥å‚æ•°ï¼ˆæ— éœ€æ‰‹åŠ¨ä¿®æ”¹ï¼‰
    IMG_SIZE = model_input_shape[1]  # è‡ªåŠ¨è·å–å°ºå¯¸ï¼ˆ48æˆ–120ï¼‰
    INPUT_CHANNELS = model_input_shape[3]  # è‡ªåŠ¨è·å–é€šé“æ•°ï¼ˆ1æˆ–3ï¼‰
    
    print(f"ğŸ” è‡ªåŠ¨é€‚é…é…ç½®:")
    print(f"  - è¾“å…¥å°ºå¯¸: {IMG_SIZE}x{IMG_SIZE}")
    print(f"  - é€šé“æ•°: {INPUT_CHANNELS} {'(ç°åº¦å›¾)' if INPUT_CHANNELS == 1 else '(RGBå›¾)'}")
    
    # åŠ è½½ YOLO äººè„¸æ£€æµ‹æ¨¡å‹
    face_model = YOLO(YOLO_MODEL_NAME)
    print("âœ… YOLOv8 äººè„¸æ£€æµ‹ç³»ç»Ÿå°±ç»ª (ä½é˜ˆå€¼æ¨¡å¼)")
    
except Exception as e:
    print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {str(e)}")
    exit()

# åˆå§‹åŒ–è¿½è¸ªå™¨å’Œæ‘„åƒå¤´
tracker = AdvancedFaceTracker()
cap = cv2.VideoCapture(0)

if not cap.isOpened():
    print("âŒ æ— æ³•æ‰“å¼€æ‘„åƒå¤´ï¼Œè¯·æ£€æŸ¥è®¾å¤‡è¿æ¥")
    exit()

# è®¾ç½®æ‘„åƒå¤´åˆ†è¾¨ç‡
cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)

print(f"\nğŸš€ ç³»ç»Ÿè¿è¡Œä¸­ | æ£€æµ‹é˜ˆå€¼: {YOLO_CONF_THRESHOLD} | IDè®°å¿†: {MAX_DISAPPEARED}å¸§")
print(f"ğŸ“Š é€‚é…åé…ç½®: {IMG_SIZE}x{IMG_SIZE} {'(ç°åº¦å›¾)' if INPUT_CHANNELS == 1 else '(RGBå›¾)'}")
print("æŒ‰ 'q' æˆ– ESC é€€å‡º")

# ================= ä¸»å¾ªç¯ï¼ˆæ™ºèƒ½é¢„å¤„ç†ï¼‰=================
while True:
    ret, frame = cap.read()
    if not ret:
        print("âš ï¸ æ— æ³•è¯»å–æ‘„åƒå¤´ç”»é¢ï¼Œæ­£åœ¨é‡è¯•...")
        continue
    frame = cv2.flip(frame, 1)  # é•œåƒç¿»è½¬
    
    # 1. YOLO äººè„¸æ£€æµ‹ï¼ˆåªæ£€æµ‹äººä½“ï¼Œç±»åˆ«0ï¼‰
    results = face_model(frame, verbose=False, conf=YOLO_CONF_THRESHOLD, classes=[0])
    rects = []
    
    if results[0].boxes is not None and len(results[0].boxes) > 0:
        boxes = results[0].boxes.data.cpu().numpy()
        for box in boxes:
            x1, y1, x2, y2 = map(int, box[:4])
            # è¿‡æ»¤è¿‡å°çš„æ¡†ï¼ˆè‡ªé€‚åº”æ¨¡å‹è¾“å…¥å°ºå¯¸ï¼‰
            min_size = int(IMG_SIZE * 0.8)
            if (x2 - x1) >= min_size and (y2 - y1) >= min_size:
                rects.append([x1, y1, x2, y2])

    # 2. äººè„¸è¿½è¸ª
    objects = tracker.update(rects)

    # 3. æƒ…ç»ªè¯†åˆ«ï¼ˆæ™ºèƒ½é¢„å¤„ç†ï¼Œè‡ªåŠ¨é€‚é…é€šé“æ•°ï¼‰
    for obj_id, data in objects.items():
        if data['disappeared'] > 0:
            continue

        x1, y1, x2, y2 = data['box']
        
        # é¢„å¤„ç†ï¼šè‡ªé€‚åº”Padding
        h_img, w_img, _ = frame.shape
        pad_ratio = 0.15 if IMG_SIZE <= 64 else 0.2  # å°å°ºå¯¸è¾“å…¥ç”¨å°Padding
        pad = int((y2 - y1) * pad_ratio)
        x1_p = max(0, x1 - pad)
        y1_p = max(0, y1 - pad)
        x2_p = min(w_img, x2 + pad)
        y2_p = min(h_img, y2 + pad)
        
        # æå–äººè„¸ROI
        face_roi = frame[y1_p:y2_p, x1_p:x2_p]
        
        if face_roi.size > 0:
            try:
                # æ™ºèƒ½é¢„å¤„ç†ï¼ˆè‡ªåŠ¨é€‚é…æ¨¡å‹è¾“å…¥ï¼‰
                if INPUT_CHANNELS == 1:
                    # æ¨¡å‹è¦æ±‚ç°åº¦å›¾ï¼ˆ1é€šé“ï¼‰
                    gray = cv2.cvtColor(face_roi, cv2.COLOR_BGR2GRAY)
                    resized = cv2.resize(gray, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_AREA)
                    normalized = resized.astype('float32') / 255.0
                    # å¢åŠ é€šé“ç»´åº¦å’Œæ‰¹æ¬¡ç»´åº¦
                    input_data = np.expand_dims(np.expand_dims(normalized, axis=-1), axis=0)
                else:
                    # æ¨¡å‹è¦æ±‚RGBå›¾ï¼ˆ3é€šé“ï¼‰
                    resized = cv2.resize(face_roi, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_AREA)
                    # ç¡®ä¿é€šé“æ•°æ­£ç¡®
                    if len(resized.shape) == 2:
                        resized = cv2.cvtColor(resized, cv2.COLOR_GRAY2RGB)
                    elif resized.shape[2] == 4:
                        resized = cv2.cvtColor(resized, cv2.COLOR_RGBA2RGB)
                    normalized = resized.astype('float32') / 255.0
                    # å¢åŠ æ‰¹æ¬¡ç»´åº¦
                    input_data = np.expand_dims(normalized, axis=0)
                
                # éªŒè¯è¾“å…¥å½¢çŠ¶ï¼ˆé¦–æ¬¡è¿è¡Œå¯å¼€å¯è°ƒè¯•ï¼‰
                # print(f"ğŸ“¥ è¾“å…¥å½¢çŠ¶: {input_data.shape}")
                
                # æƒ…ç»ªé¢„æµ‹
                preds = emotion_model.predict(input_data, verbose=0)[0]
                
                # æƒ…ç»ªå¹³æ»‘
                data['probs'].append(preds)
                min_smooth_frames = 2 if IMG_SIZE <= 64 else 3
                avg_preds = np.mean(data['probs'], axis=0) if len(data['probs']) >= min_smooth_frames else preds
                
                # è·å–æœ€ç»ˆæƒ…ç»ªå’Œç½®ä¿¡åº¦
                emotion_idx = np.argmax(avg_preds)
                data['current_label'] = EMOTIONS[emotion_idx]
                data['current_conf'] = avg_preds[emotion_idx]

            except Exception as e:
                print(f"âš ï¸ é¢„æµ‹å¤±è´¥ (ID:{obj_id}): {str(e)[:60]}")
                pass

        # 4. ç»˜åˆ¶UIç•Œé¢
        label = data['current_label']
        conf = data['current_conf']
        
        # æƒ…ç»ªé¢œè‰²æ˜ å°„
        color_map = {
            'Angry': (0, 0, 255),      # çº¢è‰²
            'Disgust': (128, 0, 128),  # ç´«è‰²
            'Fear': (255, 0, 255),     # æ´‹çº¢
            'Happy': (0, 255, 255),    # é»„è‰²
            'Sad': (255, 0, 0),        # è“è‰²
            'Surprise': (255, 165, 0), # æ©™è‰²
            'Neutral': (255, 255, 0),  # é’è‰²
            'Detecting...': (128, 128, 128) # ç°è‰²
        }
        color = color_map.get(label, (0, 255, 0))
        
        # ç»˜åˆ¶äººè„¸æ¡†
        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
        
        # ç»˜åˆ¶æ ‡ç­¾èƒŒæ™¯
        info_text = f"ID:{obj_id} {label} {int(conf*100)}%"
        t_size = cv2.getTextSize(info_text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)[0]
        bg_x2 = min(x1 + t_size[0], w_img - 5)
        cv2.rectangle(frame, (x1, y1 - 25), (bg_x2, y1), color, -1)
        
        # ç»˜åˆ¶æ–‡å­—
        cv2.putText(frame, info_text, (x1, y1 - 7), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)

    # æ˜¾ç¤ºç³»ç»ŸçŠ¶æ€
    active_faces = len([obj for obj in objects.values() if obj['disappeared'] == 0])
    mode_text = "Grayscale" if INPUT_CHANNELS == 1 else "RGB"
    stats_text = f"Active Faces: {active_faces} | {IMG_SIZE}x{IMG_SIZE}-{mode_text}"
    cv2.putText(frame, stats_text, (10, 30), 
                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)

    # æ˜¾ç¤ºç”»é¢
    cv2.imshow('Pro Emotion Detector (Auto-Adaptive)', frame)
    
    # é€€å‡ºé€»è¾‘
    key = cv2.waitKey(1)
    if key == ord('q') or key == 27:
        print("ğŸ›‘ æ­£åœ¨é€€å‡ºç³»ç»Ÿ...")
        break

# é‡Šæ”¾èµ„æº
cap.release()
cv2.destroyAllWindows()
print("ğŸ‘‹ ç³»ç»Ÿå·²å®‰å…¨é€€å‡º")